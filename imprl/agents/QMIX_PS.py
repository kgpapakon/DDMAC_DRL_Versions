import numpy as np
import torch
import torch.nn as nn

from imprl.agents.primitives.Value_agent import ValueAgent
from imprl.agents.primitives.MLP import NeuralNetwork
from imprl.agents.utils import preprocess_inputs


class QMixer(nn.Module):

    def __init__(self, n_components, state_dim, mixer_config):
        super(QMixer, self).__init__()

        """
        Mixing network for QMIX

        mixer: Q_tot = f(q_values @ w1 + b1) @ w2 + b2

        w1, b1, w2, b2 are mixing parameters generated by hypernetworks
        using the state as input. All mixing parameters are strictly positive 
        to ensure monotonicity of the mixing function.
        """

        self.n_components = n_components
        self.state_dim = state_dim
        self.hypernet_hidden_units = mixer_config["hypernet_hidden_units"]
        self.mixer_embed_dim = mixer_config["mixer_embed_dim"]

        ## Hypernetworks
        # hypernetworks for w1 and w2, depending on the number of hidden units
        if mixer_config["hypernet_hidden_units"] in [0, None]:
            self.hypernet_w1 = nn.Linear(
                self.state_dim, self.mixer_embed_dim * self.n_components
            )
            self.hypernet_w2 = nn.Linear(self.state_dim, self.mixer_embed_dim)

        elif mixer_config["hypernet_hidden_units"] > 0:
            self.hypernet_w1 = nn.Sequential(
                nn.Linear(self.state_dim, self.hypernet_hidden_units),
                nn.ReLU(),
                nn.Linear(
                    self.hypernet_hidden_units, self.mixer_embed_dim * self.n_components
                ),
            )
            self.hypernet_w2 = nn.Sequential(
                nn.Linear(self.state_dim, self.hypernet_hidden_units),
                nn.ReLU(),
                nn.Linear(self.hypernet_hidden_units, self.mixer_embed_dim),
            )
        else:
            raise ValueError("Invalid number of hidden units")

        self.hypernet_b1 = nn.Linear(self.state_dim, self.mixer_embed_dim)
        self.hypernet_b2 = nn.Sequential(
            nn.Linear(self.state_dim, self.mixer_embed_dim),
            nn.ReLU(),
            nn.Linear(self.mixer_embed_dim, 1),
        )

    def forward(self, q_values, states):

        q_values = q_values.view(-1, 1, self.n_components)

        w1 = torch.abs(self.hypernet_w1(states))
        w1 = w1.view(-1, self.n_components, self.mixer_embed_dim)

        b1 = self.hypernet_b1(states)
        b1 = b1.view(-1, 1, self.mixer_embed_dim)

        w2 = torch.abs(self.hypernet_w2(states))
        w2 = w2.view(-1, self.mixer_embed_dim, 1)

        b2 = self.hypernet_b2(states)
        b2 = b2.view(-1, 1, 1)

        # First layer
        # a = f(q @ w1 + b1)
        z = torch.bmm(q_values, w1) + b1
        a = torch.nn.functional.elu(z)

        # Second layer
        # q_tot = a @ w2 + b2
        q_tot = torch.bmm(a, w2) + b2
        q_tot = q_tot.view(-1, 1)

        return q_tot


class QMIXParameterSharing(ValueAgent):

    def __init__(self, env, config, device):

        super().__init__(env, config, device)

        self.collect_state_info = True
        self.use_state_info = config["USE_STATE_INFO"]

        self.mixer_config = config["MIXER_CONFIG"]

        # Neural networks
        n_inputs = (
            self.n_components + self.n_damage_states + 1
        )  # shape: n_components (id) + damage_states + time

        self.network_config["architecture"] = (
            [n_inputs] + self.network_config["hidden_layers"] + [self.n_comp_actions]
        )

        all_params = []
        self.q_network = NeuralNetwork(
            self.network_config["architecture"],
            initialization="orthogonal",
        ).to(device)

        all_params.extend(self.q_network.parameters())

        state_dim = self.n_components * self.n_damage_states + 1
        self.q_mixer = QMixer(self.n_components, state_dim, self.mixer_config).to(
            device
        )
        all_params.extend(self.q_mixer.parameters())

        # common optimizer
        self.optimizer = getattr(torch.optim, self.network_config["optimizer"])(
            all_params, lr=self.network_config["lr"]
        )

        # common learning rate scheduler
        lrs = self.network_config["lr_scheduler"]
        self.lr_scheduler = getattr(torch.optim.lr_scheduler, lrs["scheduler"])(
            self.optimizer, **lrs["kwargs"]
        )

        self.target_network = NeuralNetwork(self.network_config["architecture"]).to(
            device
        )
        self.target_mixer = QMixer(self.n_components, state_dim, self.mixer_config).to(
            device
        )

        # set weights equal
        self.target_network.load_state_dict(self.q_network.state_dict())
        self.target_mixer.load_state_dict(self.q_mixer.state_dict())

        # Initialization
        self.target_network_reset = config["TARGET_NETWORK_RESET"]

        self.logger = {
            "TD_loss": None,
            "learning_rate": self.network_config["lr"],
        }

    def get_random_action(self):
        action = self.env.action_space.sample()
        t_action = torch.tensor(action).to(self.device)

        return action, t_action

    def get_greedy_action(self, observation, training):

        # compute Q-values
        t_obs = preprocess_inputs(observation, 1).to(self.device)
        t_ma_obs = self.get_multiagent_obs_with_idx(t_obs).to(self.device)

        # shape: (n_components, n_comp_actions)
        q_values = self.q_network.forward(t_ma_obs, training).squeeze()
        t_action = torch.argmax(q_values, dim=-1)

        action = t_action.cpu().numpy()

        if training:
            return action, t_action
        else:
            return action

    def reset_episode(self, training=True):

        self.episode_return = 0
        self.episode += 1
        self.time = 0

        if training:

            # update exploration param
            self.exploration_param = self.exp_scheduler.step()

            # logging
            self.logger["exploration_param"] = self.exploration_param

        # if training and sufficient samples are available
        if training and self.total_time > 10 * self.batch_size:

            # set weights equal
            if self.episode % self.target_network_reset == 0:
                self.target_network.load_state_dict(self.q_network.state_dict())
                self.target_mixer.load_state_dict(self.q_mixer.state_dict())

            # update learning rate
            self.lr_scheduler.step()

            # logging
            self.logger["learning_rate"] = self.lr_scheduler.get_last_lr()[0]

    def process_experience(self, *args):

        self.process_rewards(args[-2])

        self.replay_memory.store_experience(*args)

        # start batch learning once sufficient samples are available
        if self.total_time > 10 * self.batch_size:
            sample_batch = self.replay_memory.sample_batch(self.batch_size)
            self.train(*sample_batch)

        done = args[-1]
        if done:
            self.logger["episode"] = self.episode
            self.logger["episode_cost"] = -self.episode_return

    def train(self, *args):
        loss = self.compute_loss(*args)

        # Zero gradients, perform a backward pass, and update the weights.
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # logging value update
        self.logger["TD_loss"] = loss.detach()

    def mixer(self, network, q_values, states):
        """
        q_values: shape (batch, n_components)

        states: shape (batch, n_components * n_damage_states + 1)
        """
        return network.forward(q_values, states)

    def compute_current_values(self, t_ma_obs, t_states, t_beliefs, t_actions):

        # shape: (batch_size, n_components, n_comp_actions)
        all_q_values = self.q_network.forward(t_ma_obs)

        q_values = torch.gather(all_q_values, 2, t_actions.unsqueeze(2)).squeeze()

        if self.use_state_info:
            q_total = self.mixer(self.q_mixer, q_values, t_states)
        else:
            q_total = self.mixer(self.q_mixer, q_values, t_beliefs)

        return q_total

    def get_future_values(self, t_ma_next_obs, t_next_states, t_next_beliefs):

        # compute Q-values using Q-network
        # shape: (batch_size, n_components, n_comp_actions)
        q_values = self.target_network.forward(t_ma_next_obs).detach()

        # compute argmax_a Q(s', a)
        # shape: (batch_size, n_components)
        t_best_actions = torch.argmax(q_values, dim=2)

        # compute Q-values using *target* network
        # shape: (batch_size, n_components, n_comp_actions)
        target_q_values = self.target_network.forward(t_ma_next_obs).detach()

        # select values correspoding to best actions
        # shape: (batch_size, n_components)
        future_values = torch.gather(
            target_q_values, 2, t_best_actions.unsqueeze(2)
        ).squeeze()

        if self.use_state_info:
            q_total_future = self.mixer(self.target_mixer, future_values, t_next_states)
        else:
            q_total_future = self.mixer(
                self.target_mixer, future_values, t_next_beliefs
            )

        return q_total_future.detach()

    def compute_td_target(
        self, t_ma_next_beliefs, t_next_states, t_next_beliefs, t_rewards, t_dones
    ):

        # bootstrapping
        future_values = self.get_future_values(
            t_ma_next_beliefs, t_next_states, t_next_beliefs
        )

        # set future values of done states to 0
        not_dones = 1 - t_dones
        future_values *= not_dones
        td_target = t_rewards + self.discount_factor * future_values

        return td_target.detach()

    def compute_loss(self, *args):

        (
            t_beliefs,
            t_ma_beliefs,
            t_states,
            t_actions,
            t_next_beliefs,
            t_ma_next_beliefs,
            t_next_states,
            t_rewards,
            t_dones,
        ) = self._preprocess_inputs(*args)

        td_target = self.compute_td_target(
            t_ma_next_beliefs, t_next_states, t_next_beliefs, t_rewards, t_dones
        )

        current_values = self.compute_current_values(
            t_ma_beliefs, t_states, t_beliefs, t_actions
        )

        loss = torch.nn.functional.mse_loss(current_values, td_target)

        return loss

    def _preprocess_inputs(
        self, beliefs, states, actions, next_beliefs, next_states, rewards, dones
    ):
        t_beliefs = preprocess_inputs(beliefs, self.n_components).to(self.device)
        t_next_beliefs = preprocess_inputs(next_beliefs, self.n_components).to(
            self.device
        )
        t_states = torch.from_numpy(np.stack(states)).to(self.device)
        t_next_states = torch.from_numpy(np.stack(next_states)).to(self.device)
        t_actions = torch.stack(actions).to(self.device)
        t_dones = (
            torch.tensor(np.asarray(dones).astype(int)).reshape(-1, 1).to(self.device)
        )
        t_rewards = torch.tensor(rewards).reshape(-1, 1).to(self.device)
        t_ma_beliefs = self.get_multiagent_obs_with_idx(t_beliefs, self.batch_size).to(
            self.device
        )
        t_ma_next_beliefs = self.get_multiagent_obs_with_idx(
            t_next_beliefs, self.batch_size
        ).to(self.device)

        return (
            t_beliefs,
            t_ma_beliefs,
            t_states,
            t_actions,
            t_next_beliefs,
            t_ma_next_beliefs,
            t_next_states,
            t_rewards,
            t_dones,
        )

    def save_weights(self, path, episode):
        # save q_network
        torch.save(self.q_network.state_dict(), f"{path}/q_network_{episode}.pth")
        # save q_mixer
        torch.save(self.q_mixer.state_dict(), f"{path}/q_mixer_{episode}.pth")

    def load_weights(self, path, episode):
        # load q_network
        full_path = f"{path}/q_network_{episode}.pth"
        self.q_network.load_state_dict(
            torch.load(full_path, map_location=torch.device("cpu"))
        )
        # load q_mixer
        full_path = f"{path}/q_mixer_{episode}.pth"
        self.q_mixer.load_state_dict(
            torch.load(full_path, map_location=torch.device("cpu"))
        )
